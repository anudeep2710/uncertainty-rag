# Uncertainty-Guided Hierarchical Retrieval for Token-Constrained Language Models

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ğŸ¯ Overview

This research project implements **Uncertainty-Guided Hierarchical Retrieval**, a novel approach for Small Language Models (SLMs) to navigate large documents efficiently. Unlike existing approaches (RAPTOR, MemTree) that rely on relevance scoring, our system uses **epistemic uncertainty** and **information gain** to decide when and where to "zoom in" on document content.

### Key Innovation

> **Core Hypothesis**: An SLM can navigate hierarchical document structures more efficiently by using its own predictive uncertainty as a navigation signal, stopping early when confident and diving deeper when uncertain.

## ğŸš€ Features

- **Epistemic Navigation**: Quantifies SLM uncertainty at each tree level
- **Information Gain Scoring**: Nodes scored by expected uncertainty reduction
- **Calibrated Early Stopping**: Stop traversal when confidence exceeds threshold
- **Uncertainty Traces**: Explainable retrieval paths showing decision rationale
- **Token Budget Constraint**: Works within 2-4k tokens regardless of document size

## ğŸ“Š Comparison with Existing Work

| Aspect | RAPTOR/MemTree | Our Approach |
|--------|----------------|--------------|
| Objective | Maximize relevance | **Minimize uncertainty** |
| Stopping | Budget exhausted | **Confidence threshold** |
| Scoring | LLM relevance | **Information gain** |
| Theory | Empirical | **Provable bounds** |

## ğŸ› ï¸ Installation

```bash
# Clone the repository
git clone <repo-url>
cd slm-research

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

## ğŸ“– Usage

### Quick Start

```python
from src.ingestion import ingest_document
from src.zoom_agent import UncertaintyZoomAgent
from src.llm_interface import OllamaInterface

# Initialize SLM
slm = OllamaInterface(model="llama3.2:1b")

# Ingest a document
tree = ingest_document("path/to/document.pdf", slm)

# Search with uncertainty guidance
agent = UncertaintyZoomAgent(slm)
result = agent.search(
    query="What was the learning rate in experiment 3?",
    tree=tree,
    budget=2048,
    confidence_threshold=0.8
)

print(f"Answer: {result.answer}")
print(f"Confidence: {result.confidence:.2%}")
print(f"Tokens used: {result.tokens_used}")
print(f"Expansion path: {result.expansion_path}")
```

### CLI

```bash
# Ingest a document
python cli.py ingest paper.pdf --output tree.json

# Search
python cli.py search "What is the main contribution?" --tree tree.json

# Run benchmarks
python cli.py benchmark narrativeqa --budget 2048

# Visualize tree
python cli.py visualize tree.json
```

## ğŸ§ª Running Tests

```bash
pytest tests/ -v
```

## ğŸ“ Project Structure

```
slm-research/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ fractal_tree.py      # Tree data structure
â”‚   â”œâ”€â”€ uncertainty.py       # Uncertainty estimation
â”‚   â”œâ”€â”€ zoom_agent.py        # Main algorithm
â”‚   â”œâ”€â”€ ingestion.py         # Document processing
â”‚   â””â”€â”€ llm_interface.py     # SLM API wrapper
â”œâ”€â”€ benchmarks/
â”‚   â”œâ”€â”€ framework.py         # Evaluation framework
â”‚   â”œâ”€â”€ baselines.py         # Comparison methods
â”‚   â””â”€â”€ datasets.py          # Dataset loaders
â”œâ”€â”€ tests/
â”œâ”€â”€ cli.py
â””â”€â”€ requirements.txt
```

## ğŸ“ˆ Evaluation Metrics

- **Accuracy**: Exact/fuzzy match with ground truth
- **Tokens Used**: Total input + output tokens
- **Early Stop Rate**: % queries answered before reaching leaves
- **Calibration Error**: ECE (Expected Calibration Error)
- **Uncertainty Correlation**: Does high uncertainty predict errors?

## ğŸ”¬ Research Contributions

1. **Novel framing**: Retrieval as uncertainty minimization
2. **Information-theoretic scoring**: Information gain for node selection
3. **Calibration-aware stopping**: Principled early termination
4. **Theoretical bounds**: Provable retrieval depth guarantees
5. **Interpretability**: Uncertainty traces for explainability

## ğŸ“„ License

MIT License - see LICENSE file for details.

## ğŸ™ Acknowledgments

This research builds upon ideas from:
- RAPTOR (Sarthi et al., ICLR 2024)
- MemTree (2024)
- Tree of Thoughts (Yao et al., 2023)
